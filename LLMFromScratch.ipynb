{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "v7AdZSk_L5SZ",
        "hPPawlgffddK"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdwinZhou/LLMs-from-scratch/blob/main/LLMFromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 加载 the-verdict"
      ],
      "metadata": {
        "id": "v7AdZSk_L5SZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gRv9ooqnbqlr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "e8678613-9697-405e-b760-3766301863cc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"the-verdict.txt\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "  text_data = file.read()"
      ],
      "metadata": {
        "id": "aSGzBxLw5PHz",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "e2c02415-6c69-4ac6-a8a3-d630454d4706"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'the-verdict.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4270638440.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"the-verdict.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mtext_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'the-verdict.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_data)"
      ],
      "metadata": {
        "id": "aHpzoR_HBzLI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 完整的大模型"
      ],
      "metadata": {
        "id": "OIllRdVxeEHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 多头注意力"
      ],
      "metadata": {
        "id": "hPPawlgffddK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out,\n",
        "    context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads #A\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.out_proj = nn.Linear(d_out, d_out) #B\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\n",
        "    'mask',\n",
        "    torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, d_in = x.shape\n",
        "    keys = self.W_key(x) #C\n",
        "    queries = self.W_query(x) #C\n",
        "    values = self.W_value(x) #C\n",
        "    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) #D\n",
        "    values = values.view(b, num_tokens, self.num_heads, self.head_dim) #D\n",
        "    queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)#D\n",
        "    keys = keys.transpose(1, 2) #E\n",
        "    queries = queries.transpose(1, 2) #E\n",
        "    values = values.transpose(1, 2) #E\n",
        "    attn_scores = queries @ keys.transpose(2, 3) #F\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens] #G\n",
        "    attn_scores.masked_fill_(mask_bool, -torch.inf) #H\n",
        "    attn_weights = torch.softmax(\n",
        "    attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "    context_vec = (attn_weights @ values).transpose(1, 2) #I\n",
        "    #J\n",
        "    context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "    context_vec = self.out_proj(context_vec) #K\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "TCYaPRWBfXYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## transformer 块"
      ],
      "metadata": {
        "id": "WfQODAXjfikk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.att = MultiHeadAttention(\n",
        "      d_in=cfg[\"emb_dim\"],\n",
        "      d_out=cfg[\"emb_dim\"],\n",
        "      context_length=cfg[\"context_length\"],\n",
        "      num_heads=cfg[\"n_heads\"],\n",
        "      dropout=cfg[\"drop_rate\"],\n",
        "      qkv_bias=cfg[\"qkv_bias\"])\n",
        "    self.ff = FeedForward(cfg)\n",
        "    self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "  def forward(self, x): #A\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x = self.att(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut # Add the original input back\n",
        "    shortcut = x #B\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut #C\n",
        "    return x"
      ],
      "metadata": {
        "id": "z-QcIPyYfTBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    # 定义词嵌入层（token embedding）\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "    # 定义位置嵌入层（positional embedding）：\n",
        "    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "    # 定义嵌入层 dropout，在嵌入层后添加 dropout，防止模型过度依赖某些嵌入向量，减少过拟合。\n",
        "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    self.trf_blocks = nn.Sequential(\n",
        "        *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "        )\n",
        "\n",
        "    self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "\n",
        "    self.out_head = nn.Linear(\n",
        "      cfg[\"emb_dim\"],\n",
        "      cfg[\"vocab_size\"],\n",
        "      bias=False\n",
        "    )\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, seq_len = in_idx.shape\n",
        "    tok_embeds = self.tok_emb(in_idx)\n",
        "    #A\n",
        "    pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "\n",
        "    x = tok_embeds + pos_embeds\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "F9437JeeSOwm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "  \"vocab_size\": 50257,\n",
        "  \"context_length\": 256, #A\n",
        "  \"emb_dim\": 768,\n",
        "  \"n_heads\": 12,\n",
        "  \"n_layers\": 12,\n",
        "  \"drop_rate\": 0.1, #B\n",
        "  \"qkv_bias\": False\n",
        "}\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "IO2VzgC-LZaN",
        "outputId": "98a72ea3-2ff3-4467-c7d6-1075c4a2b96c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'GPTModel' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3812498300.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     10\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPTModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGPT_CONFIG_124M\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'GPTModel' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xmL_plB44bQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 0.5 * x * (1 + torch.tanh(\n",
        "      torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "      (x + 0.044715 * torch.pow(x, 3))\n",
        "    ))\n",
        "\n",
        "gelu, relu = GELU(), nn.ReLU()\n",
        "x = torch.linspace(-3, 3, 100) #A\n",
        "y_gelu, y_relu = gelu(x), relu(x)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "      GELU(),\n",
        "      nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "ffn = FeedForward(GPT_CONFIG_124M)\n",
        "x = torch.rand(2, 3, 768) #A\n",
        "out = ffn(x)\n",
        "print(out.shape)\n",
        "\n",
        "\n",
        "def print_gradients(model, x):\n",
        "  # Forward pass\n",
        "  output = model(x)\n",
        "  target = torch.tensor([[0.]])\n",
        "  # Calculate loss based on how close the target\n",
        "  # and output are\n",
        "  loss = nn.MSELoss()\n",
        "  loss = loss(output, target)\n",
        "  # Backward pass to calculate the gradients\n",
        "  loss.backward()\n",
        "  for name, param in model.named_parameters():\n",
        "    if 'weight' in name:\n",
        "      # Print the mean absolute gradient of the weights\n",
        "      print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
        "\n",
        "class ExampleDeepNeuralNetwork(nn.Module):\n",
        "  def __init__(self, layer_sizes, use_shortcut):\n",
        "    super().__init__()\n",
        "    self.use_shortcut = use_shortcut\n",
        "    self.layers = nn.ModuleList([\n",
        "      # Implement 5 layers\n",
        "      nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
        "      nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
        "      nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
        "      nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
        "      nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
        "    ])\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "    # Compute the output of the current layer\n",
        "      layer_output = layer(x)\n",
        "    # Check if shortcut can be applied\n",
        "      if self.use_shortcut and x.shape == layer_output.shape:\n",
        "        x = x + layer_output\n",
        "      else:\n",
        "        x = layer_output\n",
        "    return x\n",
        "\n",
        "\n",
        "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
        "sample_input = torch.tensor([[1., 0., -1.]])\n",
        "torch.manual_seed(123) # specify random seed for the initial weights for reproducibility\n",
        "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
        "layer_sizes, use_shortcut=False\n",
        ")\n",
        "\n",
        "print_gradients(model_without_shortcut, sample_input)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
        "layer_sizes, use_shortcut=True\n",
        ")\n",
        "print_gradients(model_with_shortcut, sample_input)\n",
        "\n",
        "# plt.figure(figsize=(8, 3))\n",
        "# for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
        "#   plt.subplot(1, 2, i)\n",
        "#   plt.plot(x, y)\n",
        "#   plt.title(f\"{label} activation function\")\n",
        "#   plt.xlabel(\"x\")\n",
        "#   plt.ylabel(f\"{label}(x)\")\n",
        "#   plt.grid(True)\n",
        "#   plt.tight_layout()\n",
        "#   plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTvseTAM4iI_",
        "outputId": "45b93c1d-61fd-4c61-f8c2-ea59d2484611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 768])\n",
            "layers.0.0.weight has gradient mean of 0.00020173584925942123\n",
            "layers.1.0.weight has gradient mean of 0.00012011159560643137\n",
            "layers.2.0.weight has gradient mean of 0.0007152040489017963\n",
            "layers.3.0.weight has gradient mean of 0.0013988736318424344\n",
            "layers.4.0.weight has gradient mean of 0.005049645435065031\n",
            "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
            "layers.1.0.weight has gradient mean of 0.20694105327129364\n",
            "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
            "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
            "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out,\n",
        "    context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads #A\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.out_proj = nn.Linear(d_out, d_out) #B\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\n",
        "    'mask',\n",
        "    torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, d_in = x.shape\n",
        "    keys = self.W_key(x) #C\n",
        "    queries = self.W_query(x) #C\n",
        "    values = self.W_value(x) #C\n",
        "    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) #D\n",
        "    values = values.view(b, num_tokens, self.num_heads, self.head_dim) #D\n",
        "    queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)#D\n",
        "    keys = keys.transpose(1, 2) #E\n",
        "    queries = queries.transpose(1, 2) #E\n",
        "    values = values.transpose(1, 2) #E\n",
        "    attn_scores = queries @ keys.transpose(2, 3) #F\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens] #G\n",
        "    attn_scores.masked_fill_(mask_bool, -torch.inf) #H\n",
        "    attn_weights = torch.softmax(\n",
        "    attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "    context_vec = (attn_weights @ values).transpose(1, 2) #I\n",
        "    #J\n",
        "    context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "    context_vec = self.out_proj(context_vec) #K\n",
        "    return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 2\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "SJomsPdbXXvA",
        "outputId": "f7fda7c1-0a87-4517-fb77-466a6e35efb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'batch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3368029601.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0md_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mmha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GzBvCe22DhFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "第四章\n"
      ],
      "metadata": {
        "id": "rsCkeDxCSACj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import tiktoken\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "  \"vocab_size\": 50257, # Vocabulary size\n",
        "  \"context_length\": 1024, # Context length\n",
        "  \"emb_dim\": 768, # Embedding dimension\n",
        "  \"n_heads\": 12, # Number of attention heads\n",
        "  \"n_layers\": 12, # Number of layers\n",
        "  \"drop_rate\": 0.1, # Dropout rate\n",
        "  \"qkv_bias\": False # Query-Key-Value bias\n",
        "}\n",
        "\n",
        "\n",
        "class DummyGPTModel(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "    self.trf_blocks = nn.Sequential(\n",
        "      *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]) #A\n",
        "    self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"]) #B\n",
        "    self.out_head = nn.Linear(\n",
        "      cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "    )\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, seq_len = in_idx.shape\n",
        "    tok_embeds = self.tok_emb(in_idx)\n",
        "    pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "    x = tok_embeds + pos_embeds\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits\n",
        "\n",
        "\n",
        "class DummyTransformerBlock(nn.Module): #C\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x): #D\n",
        "    return x\n",
        "\n",
        "class DummyLayerNorm(nn.Module): #E\n",
        "  def __init__(self, normalized_shape, eps=1e-5): #F\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "    norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "    return self.scale * norm_x + self.shift\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "# print(batch)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = DummyGPTModel(GPT_CONFIG_124M)\n",
        "logits = model(batch)\n",
        "# print(\"Output shape:\", logits.shape)\n",
        "# print(logits)\n",
        "\n",
        "# 模拟一层传播\n",
        "torch.manual_seed(123)\n",
        "batch_example = torch.randn(2, 5) #A\n",
        "\n",
        "# layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
        "# out = layer(batch_example)\n",
        "# print(out)\n",
        "\n",
        "# 计算均值和方差\n",
        "# mean = out.mean(dim=-1, keepdim=False)\n",
        "# var = out.var(dim=-1, keepdim=False)\n",
        "# print(\"Mean:\\n\", mean)\n",
        "# print(\"Variance:\\n\", var)\n",
        "\n",
        "# 计算均值和方差，保留维度\n",
        "# mean = out.mean(dim=-1, keepdim=True)\n",
        "# var = out.var(dim=-1, keepdim=True)\n",
        "# torch.set_printoptions(sci_mode=False)\n",
        "# print(\"Mean:\\n\", mean)\n",
        "# print(\"Variance:\\n\", var)\n",
        "\n",
        "# 计算归一化之后的均值和方差\n",
        "# out_norm = (out - mean) / torch.sqrt(var)\n",
        "# mean = out_norm.mean(dim=-1, keepdim=True)\n",
        "# var = out_norm.var(dim=-1, keepdim=True)\n",
        "# print(\"Normalized layer outputs:\\n\", out_norm)\n",
        "# print(\"Mean:\\n\", mean)\n",
        "# print(\"Variance:\\n\", var)\n",
        "\n",
        "# ln = LayerNorm(emb_dim=5)\n",
        "# out_ln = ln(batch_example)\n",
        "# mean = out_ln.mean(dim=-1, keepdim=True)\n",
        "# var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
        "# print(\"Mean:\\n\", mean)\n",
        "# print(\"Variance:\\n\", var)\n",
        "\n",
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 0.5 * x * (1 + torch.tanh(\n",
        "          torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "          (x + 0.044715 * torch.pow(x, 3))\n",
        "          ))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "      GELU(),\n",
        "      nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "# ffn = FeedForward(GPT_CONFIG_124M)\n",
        "# x = torch.rand(2, 3, 768) #A\n",
        "# out = ffn(x)\n",
        "# print(out.shape)\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# gelu, relu = GELU(), nn.ReLU()\n",
        "# x = torch.linspace(-3, 3, 100) #A\n",
        "# y_gelu, y_relu = gelu(x), relu(x)\n",
        "# plt.figure(figsize=(8, 3))\n",
        "# for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
        "#   plt.subplot(1, 2, i)\n",
        "#   plt.plot(x, y)\n",
        "#   plt.title(f\"{label} activation function\")\n",
        "#   plt.xlabel(\"x\")\n",
        "#   plt.ylabel(f\"{label}(x)\")\n",
        "#   plt.grid(True)\n",
        "#   plt.tight_layout()\n",
        "#   plt.show()\n",
        "\n",
        "class ExampleDeepNeuralNetwork(nn.Module):\n",
        "  def __init__(self, layer_sizes, use_shortcut):\n",
        "    super().__init__()\n",
        "    self.use_shortcut = use_shortcut\n",
        "    self.layers = nn.ModuleList([\n",
        "      # Implement 5 layers\n",
        "      nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
        "      nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
        "      nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
        "      nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
        "      nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
        "    ])\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      # Compute the output of the current layer\n",
        "      layer_output = layer(x)\n",
        "      # Check if shortcut can be applied\n",
        "      if self.use_shortcut and x.shape == layer_output.shape:\n",
        "        x = x + layer_output\n",
        "      else:\n",
        "        x = layer_output\n",
        "\n",
        "    return x\n",
        "\n",
        "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
        "sample_input = torch.tensor([[1., 0., -1.]])\n",
        "torch.manual_seed(123) # specify random seed for the initial weights for reproducibility\n",
        "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
        "  layer_sizes, use_shortcut=True\n",
        ")\n",
        "\n",
        "\n",
        "def print_gradients(model, x):\n",
        "  # Forward pass\n",
        "  output = model(x)\n",
        "  target = torch.tensor([[0.]])\n",
        "\n",
        "  # Calculate loss based on how close the target\n",
        "  # and output are\n",
        "  loss = nn.MSELoss()\n",
        "  loss = loss(output, target)\n",
        "\n",
        "  # Backward pass to calculate the gradients\n",
        "  loss.backward()\n",
        "\n",
        "  for name, param in model.named_parameters():\n",
        "    if 'weight' in name:\n",
        "      # Print the mean absolute gradient of the weights\n",
        "      print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
        "\n",
        "# print_gradients(model_without_shortcut, sample_input)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out,\n",
        "    context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads #A\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.out_proj = nn.Linear(d_out, d_out) #B\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\n",
        "    'mask',\n",
        "    torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, d_in = x.shape\n",
        "    keys = self.W_key(x) #C\n",
        "    queries = self.W_query(x) #C\n",
        "    values = self.W_value(x) #C\n",
        "    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) #D\n",
        "    values = values.view(b, num_tokens, self.num_heads, self.head_dim) #D\n",
        "    queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)#D\n",
        "    keys = keys.transpose(1, 2) #E\n",
        "    queries = queries.transpose(1, 2) #E\n",
        "    values = values.transpose(1, 2) #E\n",
        "    attn_scores = queries @ keys.transpose(2, 3) #F\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens] #G\n",
        "    attn_scores.masked_fill_(mask_bool, -torch.inf) #H\n",
        "    attn_weights = torch.softmax(\n",
        "    attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "    context_vec = (attn_weights @ values).transpose(1, 2) #I\n",
        "    #J\n",
        "    context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "    context_vec = self.out_proj(context_vec) #K\n",
        "    return context_vec\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.att = MultiHeadAttention(\n",
        "      d_in=cfg[\"emb_dim\"],\n",
        "      d_out=cfg[\"emb_dim\"],\n",
        "      context_length=cfg[\"context_length\"],\n",
        "      num_heads=cfg[\"n_heads\"],\n",
        "      dropout=cfg[\"drop_rate\"],\n",
        "      qkv_bias=cfg[\"qkv_bias\"])\n",
        "    self.ff = FeedForward(cfg)\n",
        "    self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "  def forward(self, x): #A\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x = self.att(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut # Add the original input back\n",
        "    shortcut = x #B\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut #C\n",
        "    return x\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "x = torch.rand(2, 4, 768) #A\n",
        "block = TransformerBlock(GPT_CONFIG_124M)\n",
        "output = block(x)\n",
        "print(\"Input shape:\", x.shape)\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "id": "4dOxaB-JSC_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b33ae6a-9757-4b2c-9d37-d6840da46b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 4, 768])\n",
            "Output shape: torch.Size([2, 4, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "实现一个完整 GPT 模型"
      ],
      "metadata": {
        "id": "PSpVFEwZRV5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "out = model(batch)\n",
        "# print(\"Input batch:\\n\", batch)\n",
        "# print(\"\\nOutput shape:\", out.shape)\n",
        "# print(out)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")\n",
        "\n",
        "# print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
        "# print(\"Output layer shape:\", model.out_head.weight.shape)\n",
        "\n",
        "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
        "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")\n",
        "\n",
        "total_size_bytes = total_params * 4 #A\n",
        "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
        "print(f\"Total size of the model: {total_size_mb:.2f} MB\")\n",
        "\n",
        "def generate_text_simple(model, idx, max_new_tokens, context_size): #A\n",
        "  for _ in range(max_new_tokens):\n",
        "    idx_cond = idx[:, -context_size:] #B\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx_cond)\n",
        "\n",
        "    logits = logits[:, -1, :] #C\n",
        "    probas = torch.softmax(logits, dim=-1) #D\n",
        "    idx_next = torch.argmax(probas, dim=-1, keepdim=True) #E\n",
        "    idx = torch.cat((idx, idx_next), dim=1) #F\n",
        "\n",
        "  return idx\n",
        "\n",
        "start_context = \"Hello, I am\"\n",
        "encoded = tokenizer.encode(start_context)\n",
        "print(\"encoded:\", encoded)\n",
        "encoded_tensor = torch.tensor(encoded).unsqueeze(0) #A\n",
        "print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
        "\n",
        "model.eval() #A\n",
        "out = generate_text_simple(\n",
        "  model=model,\n",
        "  idx=encoded_tensor,\n",
        "  max_new_tokens=6,\n",
        "  context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "print(\"Output:\", out)\n",
        "print(\"Output length:\", len(out[0]))\n",
        "\n",
        "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCBYixPbRU-c",
        "outputId": "6e673188-0cf2-4c90-f827-7af9dc6c3700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 163,009,536\n",
            "Token embedding layer shape: torch.Size([50257, 768])\n",
            "Output layer shape: torch.Size([50257, 768])\n",
            "Number of trainable parameters considering weight tying: 124,412,160\n",
            "Total size of the model: 621.83 MB\n",
            "encoded: [15496, 11, 314, 716]\n",
            "encoded_tensor.shape: torch.Size([1, 4])\n",
            "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
            "Output length: 10\n",
            "Hello, I am Featureiman Byeswickattribute argue\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "第五章开始\n",
        "\n"
      ],
      "metadata": {
        "id": "9LhnEdW9aDJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import tiktoken\n",
        "\n",
        "# from chapter04 import GPTModel\n",
        "GPT_CONFIG_124M = {\n",
        "\"vocab_size\": 50257,\n",
        "\"context_length\": 256, #A\n",
        "\"emb_dim\": 768,\n",
        "\"n_heads\": 12,\n",
        "\"n_layers\": 12,\n",
        "\"drop_rate\": 0.1, #B\n",
        "\"qkv_bias\": False\n",
        "}\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "    norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "    return self.scale * norm_x + self.shift\n",
        "\n",
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 0.5 * x * (1 + torch.tanh(\n",
        "      torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "      (x + 0.044715 * torch.pow(x, 3))\n",
        "    ))\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out,\n",
        "    context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads #A\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.out_proj = nn.Linear(d_out, d_out) #B\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\n",
        "    'mask',\n",
        "    torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, d_in = x.shape\n",
        "    keys = self.W_key(x) #C\n",
        "    queries = self.W_query(x) #C\n",
        "    values = self.W_value(x) #C\n",
        "    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) #D\n",
        "    values = values.view(b, num_tokens, self.num_heads, self.head_dim) #D\n",
        "    queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)#D\n",
        "    keys = keys.transpose(1, 2) #E\n",
        "    queries = queries.transpose(1, 2) #E\n",
        "    values = values.transpose(1, 2) #E\n",
        "    attn_scores = queries @ keys.transpose(2, 3) #F\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens] #G\n",
        "    attn_scores.masked_fill_(mask_bool, -torch.inf) #H\n",
        "    attn_weights = torch.softmax(\n",
        "    attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "    context_vec = (attn_weights @ values).transpose(1, 2) #I\n",
        "    #J\n",
        "    context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "    context_vec = self.out_proj(context_vec) #K\n",
        "    return context_vec\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.att = MultiHeadAttention(\n",
        "      d_in=cfg[\"emb_dim\"],\n",
        "      d_out=cfg[\"emb_dim\"],\n",
        "      context_length=cfg[\"context_length\"],\n",
        "      num_heads=cfg[\"n_heads\"],\n",
        "      dropout=cfg[\"drop_rate\"],\n",
        "      qkv_bias=cfg[\"qkv_bias\"])\n",
        "    self.ff = FeedForward(cfg)\n",
        "    self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "  def forward(self, x): #A\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x = self.att(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut # Add the original input back\n",
        "    shortcut = x #B\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut #C\n",
        "    return x\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "      GELU(),\n",
        "      nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    # 定义词嵌入层（token embedding）\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "    # 定义位置嵌入层（positional embedding）：\n",
        "    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "    # 定义嵌入层 dropout，在嵌入层后添加 dropout，防止模型过度依赖某些嵌入向量，减少过拟合。\n",
        "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    self.trf_blocks = nn.Sequential(\n",
        "        *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "        )\n",
        "\n",
        "    self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "\n",
        "    self.out_head = nn.Linear(\n",
        "      cfg[\"emb_dim\"],\n",
        "      cfg[\"vocab_size\"],\n",
        "      bias=False\n",
        "    )\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, seq_len = in_idx.shape\n",
        "    tok_embeds = self.tok_emb(in_idx)\n",
        "    #A\n",
        "    pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "\n",
        "    x = tok_embeds + pos_embeds\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "# 切换到评估模式\n",
        "model.eval()\n",
        "\n",
        "\n",
        "def generate_text_simple(model, idx, max_new_tokens, context_size): #A\n",
        "\n",
        "  for _ in range(max_new_tokens):\n",
        "    # 这里是一个自循环机制，把上一轮生成的结果加到最开始的输出里面，作为下一轮的输入\n",
        "    idx_cond = idx[:, -context_size:] #B\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx_cond)\n",
        "\n",
        "    # 此处 logits 的形状是 [batch_size, seq_len, vocab_size]\n",
        "    # 保留最后一个 token\n",
        "    logits = logits[:, -1, :] #C\n",
        "\n",
        "    # 这里的 dim 是指 logits 的最后一个维度，logits 的最后一个维度是词汇表\n",
        "    probas = torch.softmax(logits, dim=-1) #D\n",
        "    # probas 形状仍为 [batch_size, vocab_size]\n",
        "    # keepdim=True：保持维度不变，将输出形状从 [batch_size] 变为 [batch_size, 1]（比如 [1,1]），方便后续和原始 idx 拼接\n",
        "    idx_next = torch.argmax(probas, dim=-1, keepdim=True) #E\n",
        "\n",
        "    # 合并\n",
        "    idx = torch.cat((idx, idx_next), dim=1) #F\n",
        "\n",
        "  return idx\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "  encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "  encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "  return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "  flat = token_ids.squeeze(0) # remove batch dimension\n",
        "  return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# 调用模型来生成文本\n",
        "token_ids = generate_text_simple(\n",
        "  model=model,\n",
        "  idx=text_to_token_ids(start_context, tokenizer),\n",
        "  max_new_tokens=10,\n",
        "  context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
        "\n",
        "\n",
        "inputs = torch.tensor([[16833, 3626, 6100], # [\"every effort moves\",\n",
        "[40, 1107, 588]]) # \"I really like\"]\n",
        "\n",
        "targets = torch.tensor([[3626, 6100, 345 ], # [\" effort moves you\",\n",
        "[107, 588, 11311]]) # \" really like chocolate\"]\n",
        "\n",
        "with torch.no_grad(): #A\n",
        "  logits = model(inputs)\n",
        "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
        "# print(probas.shape)\n",
        "\n",
        "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "# print(\"Token IDs:\\n\", token_ids)\n",
        "\n",
        "# print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
        "# print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n",
        "\n",
        "text_idx = 0\n",
        "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "# print(\"Text 1:\", target_probas_1)\n",
        "text_idx = 1\n",
        "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "# print(\"Text 2:\", target_probas_2)\n",
        "\n",
        "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
        "print(log_probas)\n",
        "\n",
        "avg_log_probas = torch.mean(log_probas)\n",
        "print(avg_log_probas)\n",
        "\n",
        "logits_flat = logits.flatten(0, 1)\n",
        "targets_flat = targets.flatten()\n",
        "print(\"Flattened logits:\", logits_flat.shape)\n",
        "print(\"Flattened targets:\", targets_flat.shape)\n",
        "\n",
        "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A40UX6scWCPU",
        "outputId": "87b84cc8-20ca-4b6e-88c7-2e9e8b7a0d65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n",
            "tensor([ -9.5042, -10.3796, -11.3677, -10.1492,  -9.7764, -12.2561])\n",
            "tensor(-10.5722)\n",
            "Flattened logits: torch.Size([6, 50257])\n",
            "Flattened targets: torch.Size([6])\n",
            "tensor(10.5722)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyIfCEMFuUr2",
        "outputId": "da38c3dd-d35c-431e-b47a-61efdb4bc602",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'vocab_size': 50257,\n",
              " 'context_length': 1024,\n",
              " 'emb_dim': 768,\n",
              " 'n_heads': 12,\n",
              " 'n_layers': 12,\n",
              " 'drop_rate': 0.1,\n",
              " 'qkv_bias': False}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "\"vocab_size\": 50257, # Vocabulary size\n",
        "\"context_length\": 1024, # Context length\n",
        "\"emb_dim\": 768, # Embedding dimension\n",
        "\"n_heads\": 12, # Number of attention heads\n",
        "\"n_layers\": 12, # Number of layers\n",
        "\"drop_rate\": 0.1, # Dropout rate\n",
        "\"qkv_bias\": False # Query-Key-Value bias\n",
        "}\n",
        "GPT_CONFIG_124M"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(as)"
      ],
      "metadata": {
        "id": "ptnYf4HXWB1O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "0ede0363-dcfb-4a9f-925c-00db0a551ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-3744142341.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3744142341.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print(as)\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 预训练模型\n"
      ],
      "metadata": {
        "id": "rfOyEmr2d0yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2D7yo7lR2FP",
        "outputId": "203783e0-a4aa-4097-dcb9-c41fe831764b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 预训练模型\n"
      ],
      "metadata": {
        "id": "TuJe9kyVMjdV"
      }
    }
  ]
}